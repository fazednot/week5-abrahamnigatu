{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from telethon import TelegramClient, events\n",
    "from telethon.tl.types import InputPeerChannel\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "from io import BytesIO\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from sklearn.metrics import classification_report\n",
    "import shap\n",
    "import lime\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "# ------------------------------\n",
    "# Task 1: Data Ingestion and Preprocessing\n",
    "# ------------------------------\n",
    "\n",
    "# Telegram API credentials (Replace with your actual credentials)\n",
    "api_id = 'YOUR_API_ID'\n",
    "api_hash = 'YOUR_API_HASH'\n",
    "bot_token = 'YOUR_BOT_TOKEN'\n",
    "\n",
    "# Initialize Telegram client\n",
    "client = TelegramClient('ethiomart_session', api_id, api_hash).start(bot_token=bot_token)\n",
    "\n",
    "# List of Telegram channel usernames or IDs\n",
    "channels = ['Shageronlinestore', 'AnotherEcommerceChannel']\n",
    "\n",
    "# Directory to store fetched data\n",
    "data_dir = 'raw'\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Function to preprocess Amharic text\n",
    "def preprocess_amharic(text):\n",
    "    # Normalize text: remove unwanted characters, handle diacritics\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Function to download and preprocess images using OCR\n",
    "def preprocess_image(image_bytes):\n",
    "    image = Image.open(BytesIO(image_bytes))\n",
    "    text = pytesseract.image_to_string(image, lang='amh')  # Ensure Amharic OCR is supported\n",
    "    return preprocess_amharic(text)\n",
    "\n",
    "# Event handler for new messages\n",
    "@client.on(events.NewMessage(chats=channels))\n",
    "async def handler(event):\n",
    "    message = event.message.message\n",
    "    sender = await event.get_sender()\n",
    "    sender_id = sender.id\n",
    "    timestamp = event.message.date.isoformat()\n",
    "    media = event.message.media\n",
    "\n",
    "    # Initialize data dictionary\n",
    "    data = {\n",
    "        'sender_id': sender_id,\n",
    "        'timestamp': timestamp,\n",
    "        'message': message,\n",
    "        'entities': {}\n",
    "    }\n",
    "\n",
    "    # Handle media (images)\n",
    "    if media:\n",
    "        if hasattr(media, 'photo'):\n",
    "            image = await event.download_media(media)\n",
    "            with open(os.path.join(data_dir, f\"{sender_id}_{int(time.time())}.jpg\"), 'wb') as f:\n",
    "                f.write(image)\n",
    "            # Extract text from image\n",
    "            ocr_text = preprocess_image(image)\n",
    "            data['message'] += ' ' + ocr_text\n",
    "\n",
    "    # Preprocess text\n",
    "    clean_text = preprocess_amharic(message)\n",
    "    data['message'] = clean_text\n",
    "\n",
    "    # Save data to JSON\n",
    "    with open(os.path.join(data_dir, f\"{sender_id}_{int(time.time())}.json\"), 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Saved message from {sender_id} at {timestamp}\")\n",
    "\n",
    "# Start the client\n",
    "print(\"Starting Telegram client...\")\n",
    "client.run_until_disconnected()\n",
    "\n",
    "# ------------------------------\n",
    "# Task 2: Labeling Dataset in CoNLL Format\n",
    "# ------------------------------\n",
    "\n",
    "messages_df = pd.read_csv('messages.csv')  # Replace with your actual file path\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Task 3: Fine-Tuning the NER Model\n",
    "# ------------------------------\n",
    "\n",
    "# Load the labeled dataset\n",
    "def load_conll_data(file_path):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    sentence = []\n",
    "    label = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip() == \"\":\n",
    "                if sentence:\n",
    "                    sentences.append(sentence)\n",
    "                    labels.append(label)\n",
    "                    sentence = []\n",
    "                    label = []\n",
    "            else:\n",
    "                token, tag = line.strip().split()\n",
    "                sentence.append(token)\n",
    "                label.append(tag)\n",
    "    return sentences, labels\n",
    "\n",
    "train_sentences, train_labels = load_conll_data('data/labeled/train.conll')\n",
    "val_sentences, val_labels = load_conll_data('data/labeled/val.conll')\n",
    "\n",
    "# Create Hugging Face Dataset\n",
    "train_dataset = Dataset.from_dict({'tokens': train_sentences, 'ner_tags': train_labels})\n",
    "val_dataset = Dataset.from_dict({'tokens': val_sentences, 'ner_tags': val_labels})\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset\n",
    "})\n",
    "\n",
    "# Define label list\n",
    "label_list = ['O', 'B-Product', 'I-Product', 'B-LOC', 'I-LOC', 'B-PRICE', 'I-PRICE']\n",
    "label_to_id = {label: idx for idx, label in enumerate(label_list)}\n",
    "id_to_label = {idx: label for label, idx in label_to_id.items()}\n",
    "\n",
    "# Tokenizer and Model\n",
    "model_name = \"xlm-roberta-base\"  # You can choose other models like bert-tiny-amharic if available\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(label_list))\n",
    "\n",
    "# Tokenize the data\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples['tokens'], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['ner_tags']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label_to_id[label[word_idx]])\n",
    "            else:\n",
    "                label_ids.append(label_to_id[label[word_idx]] if label[word_idx].startswith('I-') else -100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "# Define Metrics\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels = [[id_to_label[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [id_to_label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    report = classification_report(true_labels, true_predictions, output_dict=True, zero_division=0)\n",
    "    return {\n",
    "        'precision': report['weighted avg']['precision'],\n",
    "        'recall': report['weighted avg']['recall'],\n",
    "        'f1': report['weighted avg']['f1-score']\n",
    "    }\n",
    "\n",
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Fine-Tune the Model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the Model\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Validation F1 Score: {eval_results['eval_f1']}\")\n",
    "\n",
    "# Save the Fine-Tuned Model\n",
    "trainer.save_model(\"fine-tuned-ner-amharic\")\n",
    "tokenizer.save_pretrained(\"fine-tuned-ner-amharic\")\n",
    "\n",
    "# ------------------------------\n",
    "# Task 4: Model Comparison & Selection\n",
    "# ------------------------------\n",
    "\n",
    "# Define multiple models to compare\n",
    "models_to_compare = {\n",
    "    'xlm-roberta-base': \"xlm-roberta-base\",\n",
    "    'distilbert-base-multilingual-cased': \"distilbert-base-multilingual-cased\",\n",
    "    'bert-base-multilingual-cased': \"bert-base-multilingual-cased\",\n",
    "    # Add more models as needed\n",
    "}\n",
    "\n",
    "model_performance = {}\n",
    "\n",
    "for model_key, model_name in models_to_compare.items():\n",
    "    print(f\"Fine-tuning model: {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(label_list))\n",
    "    \n",
    "    # Tokenize\n",
    "    tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)\n",
    "    \n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset['train'],\n",
    "        eval_dataset=tokenized_dataset['validation'],\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    # Fine-Tune\n",
    "    trainer.train()\n",
    "    \n",
    "    # Evaluate\n",
    "    eval_results = trainer.evaluate()\n",
    "    f1 = eval_results['eval_f1']\n",
    "    model_performance[model_key] = f1\n",
    "    print(f\"Model: {model_key}, F1 Score: {f1}\")\n",
    "\n",
    "# Select the best-performing model\n",
    "best_model_name = max(model_performance, key=model_performance.get)\n",
    "print(f\"Best model selected: {best_model_name} with F1 Score: {model_performance[best_model_name]}\")\n",
    "\n",
    "# ------------------------------\n",
    "# Task 5: Model Interpretability\n",
    "# ------------------------------\n",
    "\n",
    "# Load the best model\n",
    "best_model = AutoModelForTokenClassification.from_pretrained(f\"fine-tuned-ner-amharic\")\n",
    "best_tokenizer = AutoTokenizer.from_pretrained(f\"fine-tuned-ner-amharic\")\n",
    "\n",
    "# Function to predict entities\n",
    "def predict_entities(text):\n",
    "    inputs = best_tokenizer(text, return_tensors=\"pt\")\n",
    "    outputs = best_model(**inputs).logits\n",
    "    predictions = torch.argmax(outputs, dim=2)\n",
    "    tokens = best_tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    entities = []\n",
    "    for token, prediction in zip(tokens, predictions[0].tolist()):\n",
    "        label = id_to_label[prediction]\n",
    "        entities.append((token, label))\n",
    "    return entities\n",
    "\n",
    "# Example usage\n",
    "example_text = \"ዋጋ 1000 ብር ያለው ቤቲ ቦትል በቦሌ አዲስ አበባ\"\n",
    "entities = predict_entities(example_text)\n",
    "print(entities)\n",
    "\n",
    "# SHAP Interpretability\n",
    "# Note: SHAP for transformers is complex; here is a simplified example\n",
    "\n",
    "# Define a prediction function for SHAP\n",
    "def model_predict(texts):\n",
    "    inputs = best_tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = best_model(**inputs).logits\n",
    "    return outputs.numpy()\n",
    "\n",
    "explainer = shap.Explainer(model_predict, best_tokenizer)\n",
    "shap_values = explainer([example_text])\n",
    "shap.summary_plot(shap_values, feature_names=best_tokenizer.tokenize(example_text))\n",
    "\n",
    "# LIME Interpretability\n",
    "explainer = LimeTextExplainer(class_names=label_list)\n",
    "\n",
    "def predict_proba(texts):\n",
    "    inputs = best_tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = best_model(**inputs).logits\n",
    "    probs = torch.softmax(outputs, dim=2)\n",
    "    return probs.numpy()\n",
    "\n",
    "exp = explainer.explain_instance(example_text, predict_proba, num_features=10)\n",
    "exp.show_in_notebook(text=True)\n",
    "\n",
    "# Save interpretability plots if needed\n",
    "# shap.summary_plot(shap_values, feature_names=best_tokenizer.tokenize(example_text), show=False)\n",
    "# plt.savefig('shap_summary.png')\n",
    "# exp.save_to_file('lime_explanation.html')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
